{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yenngoc1809/edm-student-prediction/blob/main/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Itl1hURoO6pP",
        "outputId": "9baa3440-fc25-4f85-f853-2f29c90ba0f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n",
            "Collecting fairlearn\n",
            "  Downloading fairlearn-0.12.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.11/dist-packages (from fairlearn) (1.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.3->fairlearn) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)\n",
            "Downloading fairlearn-0.12.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.12.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "%pip install optuna\n",
        "%pip install fairlearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-c0XagMo01u"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDcSoK2xo0fY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_P4ZEe8TA83"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/data_encoded.csv\"\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jbgR1ISTJyG"
      },
      "source": [
        "## **split data and balance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjVeoc1vTPjN",
        "outputId": "1c73aecf-9ef4-474d-eb5c-4dd9937f9603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE:\n",
            "Academic_Performance\n",
            "1    702\n",
            "0     94\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution after SMOTE:\n",
            "Academic_Performance\n",
            "1    698\n",
            "0    698\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from imblearn.combine import SMOTETomek # Import SMOTETomek\n",
        "\n",
        "X = data.drop(columns=['Academic_Performance']) # Independent variables\n",
        "y = data['Academic_Performance']  # Target variable\n",
        "\n",
        "\n",
        "# Split data first (to avoid leakage)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Print class distribution before balancing\n",
        "print(\"Class distribution before SMOTE:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "\n",
        "# Apply SMOTETomek to balance classes and clean overlaps\n",
        "smote_tomek = SMOTETomek(sampling_strategy=1.0, random_state=42, n_jobs=-1)\n",
        "X_train_balanced, y_train_balanced = smote_tomek.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print class distribution after balancing\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(y_train_balanced.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSAe_4PNNc4k"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Count class distribution after balancing\n",
        "# counts = y_train_balanced.value_counts().sort_index()\n",
        "\n",
        "# # Create labels for the pie chart\n",
        "# labels = ['High/Avg Performance (0)', 'Low Performance (1)']\n",
        "# colors = ['#66b3ff', '#ff9999']\n",
        "\n",
        "# plt.figure(figsize=(6, 6))\n",
        "# plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors, explode=(0.05, 0.05))\n",
        "# plt.title('Academic Performance Distribution After SMOTE')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bew6ToISVOul"
      },
      "outputs": [],
      "source": [
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.manifold import TSNE\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# import numpy as np # Import numpy\n",
        "\n",
        "# # Define SEED for reproducibility\n",
        "# SEED = 42\n",
        "# np.random.seed(SEED) # Set numpy seed for reproducibility\n",
        "\n",
        "# # Ensure X_train_balanced and y_train_balanced are available\n",
        "# # These are created in cell KjVeoc1vTPjN\n",
        "\n",
        "# # Scale the balanced training data\n",
        "# # It's generally better to scale *after* SMOTE\n",
        "# scaler_tsne = StandardScaler()\n",
        "# X_train_balanced_scaled = scaler_tsne.fit_transform(X_train_balanced)\n",
        "\n",
        "\n",
        "# # Perform t-SNE for dimensionality reduction\n",
        "# # Reduce dimensions to 2 for visualization\n",
        "# tsne = TSNE(n_components=2, random_state=SEED, perplexity=30, n_iter=300)\n",
        "# X_train_tsne = tsne.fit_transform(X_train_balanced_scaled)\n",
        "\n",
        "# # Create a DataFrame for plotting\n",
        "# tsne_df = pd.DataFrame(data=X_train_tsne, columns=['TSNE_1', 'TSNE_2'])\n",
        "# # Ensure y_train_balanced is a Series or array before accessing .values\n",
        "# if isinstance(y_train_balanced, pd.Series):\n",
        "#     tsne_df['Academic_Performance'] = y_train_balanced.values\n",
        "# else:\n",
        "#     tsne_df['Academic_Performance'] = y_train_balanced\n",
        "\n",
        "\n",
        "# # Visualize the t-SNE results\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# sns.scatterplot(\n",
        "#     x='TSNE_1', y='TSNE_2',\n",
        "#     hue='Academic_Performance',\n",
        "#     palette=sns.color_palette('hsv', 2), # Assuming 2 classes\n",
        "#     data=tsne_df,\n",
        "#     legend='full',\n",
        "#     alpha=0.5\n",
        "# )\n",
        "# plt.title('t-SNE visualization of Balanced Training Data')\n",
        "# plt.xlabel('TSNE 1')\n",
        "# plt.ylabel('TSNE 2')\n",
        "# plt.show()\n",
        "\n",
        "# # Optional: You can also try PCA if t-SNE takes too long or for comparison\n",
        "# # pca = PCA(n_components=2, random_state=SEED)\n",
        "# # X_train_pca = pca.fit_transform(X_train_balanced_scaled)\n",
        "\n",
        "# # pca_df = pd.DataFrame(data=X_train_pca, columns=['PCA_1', 'PCA_2'])\n",
        "# # if isinstance(y_train_balanced, pd.Series):\n",
        "# #     pca_df['Academic_Performance'] = y_train_balanced.values\n",
        "# # else:\n",
        "# #     pca_df['Academic_Performance'] = y_train_balanced\n",
        "\n",
        "# # plt.figure(figsize=(8, 6))\n",
        "# # sns.scatterplot(\n",
        "# #     x='PCA_1', y='PCA_2',\n",
        "# #     hue='Academic_Performance',\n",
        "# #     palette=sns.color_palette('hsv', 2),\n",
        "# #     data=pca_df,\n",
        "# #     legend='full',\n",
        "# #     alpha=0.5\n",
        "# # )\n",
        "# # plt.title('PCA visualization of Balanced Training Data')\n",
        "# # plt.xlabel('PCA 1')\n",
        "# # plt.ylabel('PCA 2')\n",
        "# # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1cfadf7"
      },
      "source": [
        "# Tabnet\n",
        "Generate Python code to train and evaluate a TabNet model with Optuna hyperparameter tuning and early stopping, including data preparation, model definition, Optuna objective function, training, evaluation, and visualization of results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb54dcc"
      },
      "source": [
        "## Import libraries\n",
        "\n",
        "### Subtask:\n",
        "Import necessary libraries for TabNet (from `pytorch_tabnet`), Optuna, PyTorch, data handling, splitting, scaling, balancing, and evaluation metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459ad687"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries for TabNet and related data science tasks as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7db2e0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `pytorch_tabnet` library is not installed. Install the missing library using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Wa7eIpzNWb9Z",
        "outputId": "7490a482-629f-4e89-a7c1-99a1ba2d5f3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.0.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.6.1)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.11/dist-packages (from pytorch-tabnet) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pytorch-tabnet-4.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pytorch-tabnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79924cf4"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `pytorch-tabnet` is confirmed to be installed, import the necessary libraries for TabNet and related data science tasks as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50997592"
      },
      "outputs": [],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import optuna\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f27460f"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "### Subtask:\n",
        "Ensure the data is in the correct format (NumPy arrays) and scaled appropriately for TabNet. This might involve reusing or adapting the scaling from the ANN section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72fab642"
      },
      "source": [
        "**Reasoning**:\n",
        "Check the data types and formats of `X_train_balanced`, `X_test`, `y_train_balanced`, and `y_test`, and convert them to NumPy arrays if necessary, ensuring `X_train` and `X_test` are scaled using the existing scaler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3d1156c",
        "outputId": "9394c70f-f5bd-4eb3-8f22-d54000242538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation for TabNet complete:\n",
            "X_train_balanced_scaled shape: (1396, 14), dtype: float64\n",
            "y_train_balanced_int shape: (1396,), dtype: int64\n",
            "X_test_scaled shape: (200, 14), dtype: float64\n",
            "y_test_int shape: (200,), dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Re-initialize and fit the scaler using the original X_train data\n",
        "# X_train was defined before SMOTE in cell KjVeoc1vTPjN\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled_original = scaler.fit_transform(X_train)\n",
        "\n",
        "# Now proceed with data preparation for TabNet, ensuring scaling is applied correctly\n",
        "\n",
        "# Check data types and convert to NumPy arrays if needed\n",
        "if not isinstance(X_train_balanced, np.ndarray):\n",
        "    X_train_balanced_np = X_train_balanced.values\n",
        "else:\n",
        "    X_train_balanced_np = X_train_balanced\n",
        "\n",
        "if not isinstance(X_test, np.ndarray):\n",
        "    X_test_np = X_test.values\n",
        "else:\n",
        "    X_test_np = X_test\n",
        "\n",
        "if not isinstance(y_train_balanced, np.ndarray):\n",
        "    y_train_balanced_np = y_train_balanced.values\n",
        "else:\n",
        "    y_train_balanced_np = y_train_balanced\n",
        "\n",
        "if not isinstance(y_test, np.ndarray):\n",
        "    y_test_np = y_test.values\n",
        "else:\n",
        "    y_test_np = y_test\n",
        "\n",
        "# Apply the scaling using the fitted scaler\n",
        "# Note: scaler.transform expects a 2D array.\n",
        "X_train_balanced_scaled = scaler.transform(X_train_balanced_np)\n",
        "X_test_scaled = scaler.transform(X_test_np)\n",
        "\n",
        "# Ensure target variables are integer type\n",
        "y_train_balanced_int = y_train_balanced_np.astype(int)\n",
        "y_test_int = y_test_np.astype(int)\n",
        "\n",
        "print(\"Data preparation for TabNet complete:\")\n",
        "print(f\"X_train_balanced_scaled shape: {X_train_balanced_scaled.shape}, dtype: {X_train_balanced_scaled.dtype}\")\n",
        "print(f\"y_train_balanced_int shape: {y_train_balanced_int.shape}, dtype: {y_train_balanced_int.dtype}\")\n",
        "print(f\"X_test_scaled shape: {X_test_scaled.shape}, dtype: {X_test_scaled.dtype}\")\n",
        "print(f\"y_test_int shape: {y_test_int.shape}, dtype: {y_test_int.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b64d88c"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `scaler` object was not found. This means the cell where the `StandardScaler` was initialized and fitted (cell REomaJgBVkWX, which was part of the ANN section) was not executed or the variable was not persistent. I need to re-initialize and fit the scaler using the original `X_train` data (before SMOTE) to ensure the scaling is applied correctly and consistently to both the balanced training data and the test data. Then, I will proceed with checking data types and applying the scaling as originally intended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29351a7"
      },
      "source": [
        "## Define objective function\n",
        "\n",
        "### Subtask:\n",
        "Create an Optuna objective function for TabNet to optimize hyperparameters based on validation accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87453273"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the Optuna objective function for TabNet hyperparameter tuning, including splitting the data, instantiating and training the model with early stopping, and returning the validation accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6d5572e"
      },
      "outputs": [],
      "source": [
        "# 1) Ensure reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # --- Assume X, y are your features & labels as numpy arrays ---\n",
        "# # 2) Train/test split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y, test_size=0.2, stratify=y, random_state=SEED\n",
        "# )\n",
        "\n",
        "# 3) Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# 4) Balance train via SMOTE\n",
        "sm = SMOTE(random_state=SEED)\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter suggestions for TabNet\n",
        "    n_d = trial.suggest_int(\"n_d\", 8, 64)\n",
        "    n_a = trial.suggest_int(\"n_a\", 8, 64)\n",
        "    n_steps = trial.suggest_int(\"n_steps\", 3, 10)\n",
        "    gamma = trial.suggest_float(\"gamma\", 1.0, 2.0)\n",
        "    lambda_sparse = trial.suggest_float(\"lambda_sparse\", 1e-6, 1e-3, log=True)\n",
        "    optimizer_fn = trial.suggest_categorical(\"optimizer_fn\", [torch.optim.Adam, torch.optim.RMSprop])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    optimizer_params = {\"lr\": lr}\n",
        "    mask_type = trial.suggest_categorical(\"mask_type\", [\"sparsemax\", \"entmax\"])\n",
        "\n",
        "    scheduler_fn = trial.suggest_categorical(\n",
        "        \"scheduler_fn\",\n",
        "        [torch.optim.lr_scheduler.StepLR, torch.optim.lr_scheduler.ReduceLROnPlateau]\n",
        "    )\n",
        "    if scheduler_fn == torch.optim.lr_scheduler.StepLR:\n",
        "        scheduler_params = {\n",
        "            \"step_size\": trial.suggest_int(\"step_size\", 10, 50),\n",
        "            \"gamma\": trial.suggest_float(\"scheduler_gamma\", 0.1, 0.9)\n",
        "        }\n",
        "    else:  # ReduceLROnPlateau\n",
        "        scheduler_params = {\n",
        "            \"mode\": \"min\",\n",
        "            \"factor\": trial.suggest_float(\"scheduler_factor\", 0.1, 0.5),\n",
        "            \"patience\": trial.suggest_int(\"scheduler_patience\", 5, 15),\n",
        "            \"verbose\": False\n",
        "        }\n",
        "\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "\n",
        "    # --- Cross-validation ---\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    cv_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train_bal, y_train_bal):\n",
        "        X_sub, X_val = X_train_bal[train_idx], X_train_bal[val_idx]\n",
        "        y_sub, y_val = y_train_bal[train_idx], y_train_bal[val_idx]\n",
        "\n",
        "        # Instantiate TabNet model\n",
        "        model = TabNetClassifier(\n",
        "            n_d=n_d,\n",
        "            n_a=n_a,\n",
        "            n_steps=n_steps,\n",
        "            gamma=gamma,\n",
        "            lambda_sparse=lambda_sparse,\n",
        "            optimizer_fn=optimizer_fn,\n",
        "            optimizer_params=optimizer_params,\n",
        "            mask_type=mask_type,\n",
        "            scheduler_fn=scheduler_fn,\n",
        "            scheduler_params=scheduler_params,\n",
        "            seed=SEED,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Fit\n",
        "        model.fit(\n",
        "            X_sub, y_sub,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            patience=20,\n",
        "            max_epochs=200,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        acc = accuracy_score(y_val, y_val_pred)\n",
        "        cv_scores.append(acc)\n",
        "\n",
        "    # Return mean accuracy across folds\n",
        "    return np.mean(cv_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "209a5b7e"
      },
      "source": [
        "## Run optuna study\n",
        "\n",
        "### Subtask:\n",
        "Execute the Optuna study to find the best hyperparameters for the TabNet model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cae55f2"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the Optuna study to find the best hyperparameters for the TabNet model using the defined objective function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364,
          "referenced_widgets": [
            "caa542e0a3344ac6a20e47b29876bb1a",
            "d5b65f879aae419481e4eca9710fbc89",
            "67e3c50ef02d408385322d624145935a",
            "1640e4c8723d4f399079967173ab632b",
            "0e172a0b57524e079b1fa15b24cefb5d",
            "8352a822f5c24067b92221ae95907ae6",
            "61daec16a49b4a009e99329031b9d73e",
            "6996fd562c834bd289d1ae8c434e09db",
            "c5d4fadee42b4c4db3eab5d5f9b94918",
            "eee1882483224fccb7451509c6a35a71",
            "5a5ec3d0330547138a41796b2d4bb229"
          ]
        },
        "id": "2e0ddfa5",
        "outputId": "a8e66744-bd7e-4bf9-976b-20ea2e87da44"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-08-18 08:30:49,419] A new study created in memory with name: no-name-db39d778-ea69-4038-817d-2601696fd156\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caa542e0a3344ac6a20e47b29876bb1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.rmsprop.RMSprop'> which is of type type.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.lr_scheduler.StepLR'> which is of type type.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.lr_scheduler.ReduceLROnPlateau'> which is of type type.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Early stopping occurred at epoch 59 with best_epoch = 39 and best_val_0_auc = 0.93891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "study_tabnet = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
        "study_tabnet.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(\"Best parameters for TabNet:\", study_tabnet.best_params)\n",
        "print(f\"Best validation accuracy for TabNet: {study_tabnet.best_value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2977e847"
      },
      "source": [
        "## Train Final Model and Evaluate\n",
        "\n",
        "### Subtask:\n",
        "Train the TabNet model on the full balanced training data using the best hyperparameters and evaluate it on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee666f42"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the final TabNet model using the best hyperparameters found by Optuna and evaluate its performance on the test set, providing accuracy, confusion matrix, classification report, and ROC-AUC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bb176f3"
      },
      "outputs": [],
      "source": [
        "# Train final model on full balanced train split using best params\n",
        "best_params_tabnet = study_tabnet.best_params\n",
        "\n",
        "# Need to handle optimizer_fn and scheduler_fn which are types, not instances\n",
        "best_params_tabnet_instantiated = best_params_tabnet.copy()\n",
        "best_params_tabnet_instantiated['optimizer_fn'] = best_params_tabnet_instantiated['optimizer_fn']\n",
        "best_params_tabnet_instantiated['scheduler_fn'] = best_params_tabnet_instantiated['scheduler_fn']\n",
        "# Remove batch_size as it's not a model parameter\n",
        "batch_size_final = best_params_tabnet_instantiated.pop('batch_size')\n",
        "\n",
        "\n",
        "final_model_tabnet = TabNetClassifier(\n",
        "    n_d=best_params_tabnet_instantiated['n_d'],\n",
        "    n_a=best_params_tabnet_instantiated['n_a'],\n",
        "    n_steps=best_params_tabnet_instantiated['n_steps'],\n",
        "    gamma=best_params_tabnet_instantiated['gamma'],\n",
        "    lambda_sparse=best_params_tabnet_instantiated['lambda_sparse'],\n",
        "    optimizer_fn=best_params_tabnet_instantiated['optimizer_fn'],\n",
        "    optimizer_params={'lr': best_params_tabnet_instantiated['lr']}, # Pass lr correctly\n",
        "    mask_type=best_params_tabnet_instantiated['mask_type'],\n",
        "    scheduler_fn=best_params_tabnet_instantiated['scheduler_fn'],\n",
        "    scheduler_params={'step_size': best_params_tabnet_instantiated['step_size'], 'gamma': best_params_tabnet_instantiated['scheduler_gamma']} if best_params_tabnet_instantiated['scheduler_fn'] == torch.optim.lr_scheduler.StepLR else {'mode': 'min', 'factor': best_params_tabnet_instantiated['scheduler_factor'], 'patience': best_params_tabnet_instantiated['scheduler_patience'], 'verbose': False},\n",
        "    seed=SEED,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "\n",
        "# Train the final model (optional: add early stopping here too based on a validation set if desired)\n",
        "# For simplicity, we'll train for a fixed number of epochs here, or you could re-split for final validation.\n",
        "# Using the same patience as in Optuna objective\n",
        "final_model_tabnet.fit(\n",
        "    X_train_balanced_scaled, y_train_balanced_int,\n",
        "    eval_set=[(X_test_scaled, y_test_int)], # Using test set for early stopping in final model training (be cautious of data leakage)\n",
        "    patience=20,\n",
        "    max_epochs=200,\n",
        "    batch_size=batch_size_final\n",
        ")\n",
        "\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_tabnet = final_model_tabnet.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "print(f\"\\nTabNet Test Accuracy: {accuracy_score(y_test_int, y_pred_tabnet):.4f}\")\n",
        "print(\"\\nTabNet Confusion Matrix:\\n\", confusion_matrix(y_test_int, y_pred_tabnet))\n",
        "print(\"\\nTabNet Classification Report:\\n\", classification_report(y_test_int, y_pred_tabnet))\n",
        "\n",
        "# ROC–AUC\n",
        "y_prob_tabnet = final_model_tabnet.predict_proba(X_test_scaled)[:, 1]\n",
        "auc_tabnet = roc_auc_score(y_test_int, y_prob_tabnet)\n",
        "fpr_tabnet, tpr_tabnet, _ = roc_curve(y_test_int, y_prob_tabnet)\n",
        "print(f\"\\nTabNet Test ROC–AUC: {auc_tabnet:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "plt.plot(fpr_tabnet, tpr_tabnet, label=f'AUC = {auc_tabnet:.2f}')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "plt.title('ROC Curve - TabNet')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFG5vKIf2Kbb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, MetricFrame\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# True labels (test set)\n",
        "true_labels_test = y_test_int\n",
        "\n",
        "# Predictions từ TabNet\n",
        "predicted_labels_test = y_pred_tabnet\n",
        "\n",
        "# --- Metrics ---\n",
        "metrics = {\n",
        "    \"accuracy\": accuracy_score,\n",
        "    \"selection_rate\": lambda y_true, y_pred: np.asarray(y_pred).mean(),\n",
        "    \"true_positive_rate\": lambda y_true, y_pred: classification_report(\n",
        "        y_true, y_pred, output_dict=True).get(\"1\", {}).get(\"recall\", 0.0),\n",
        "    \"false_positive_rate\": lambda y_true, y_pred: (\n",
        "        confusion_matrix(np.asarray(y_true), np.asarray(y_pred)).ravel()[1] /\n",
        "        (confusion_matrix(np.asarray(y_true), np.asarray(y_pred)).ravel()[1] +\n",
        "         confusion_matrix(np.asarray(y_true), np.asarray(y_pred)).ravel()[0])\n",
        "        if confusion_matrix(np.asarray(y_true), np.asarray(y_pred)).ravel().sum() > 0 else 0.0\n",
        "    )\n",
        "}\n",
        "\n",
        "# --- Sensitive feature: Gender ---\n",
        "sensitive_features_gender_test = X_test[\"Gender\"]\n",
        "\n",
        "print(\"\\n--- Fairness Evaluation (Sensitive Attribute: Gender) ---\")\n",
        "\n",
        "# MetricFrame\n",
        "metric_frame_gender = MetricFrame(\n",
        "    metrics=metrics,\n",
        "    y_true=true_labels_test,\n",
        "    y_pred=predicted_labels_test,\n",
        "    sensitive_features=sensitive_features_gender_test\n",
        ")\n",
        "\n",
        "# Metrics theo từng Gender group\n",
        "print(\"\\nMetrics by Gender Group:\")\n",
        "display(metric_frame_gender.by_group)\n",
        "\n",
        "# ΔMetrics (chênh lệch giữa nhóm Gender)\n",
        "print(\"\\n--- ΔMetrics (Disparities between Gender Groups) ---\")\n",
        "for metric_name in metrics.keys():\n",
        "    delta_value = (\n",
        "        metric_frame_gender.by_group[metric_name].max()\n",
        "        - metric_frame_gender.by_group[metric_name].min()\n",
        "    )\n",
        "    print(f\"Δ{metric_name}: {delta_value:.4f}\")\n",
        "\n",
        "# Fairness metrics chuẩn (từ fairlearn)\n",
        "dp_diff_gender = demographic_parity_difference(\n",
        "    y_true=true_labels_test,\n",
        "    y_pred=predicted_labels_test,\n",
        "    sensitive_features=sensitive_features_gender_test\n",
        ")\n",
        "print(f\"\\nDemographic parity difference (Gender): {dp_diff_gender:.4f}\")\n",
        "\n",
        "eo_diff_gender = equalized_odds_difference(\n",
        "    y_true=true_labels_test,\n",
        "    y_pred=predicted_labels_test,\n",
        "    sensitive_features=sensitive_features_gender_test\n",
        ")\n",
        "print(f\"Equalized odds difference (Gender): {eo_diff_gender:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb7aon37H_s3"
      },
      "source": [
        "# **ANN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9qN9lpkIIV8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import optuna\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix, classification_report,\n",
        "    roc_auc_score, roc_curve\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Ensure reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # --- Assume X, y are your features & labels as numpy arrays ---\n",
        "# # 2) Train/test split\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y, test_size=0.2, stratify=y, random_state=SEED\n",
        "# )\n",
        "\n",
        "# 3) Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# 4) Balance train via SMOTE\n",
        "sm = SMOTE(random_state=SEED)\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 5) Define PyTorch model class\n",
        "# ----------------------------------------------------------------\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_size, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_features = input_dim\n",
        "        # Add n_layers of (Linear → BN → ReLU → Dropout)\n",
        "        for i in range(n_layers):\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "            in_features = hidden_size\n",
        "        # Final output layer\n",
        "        layers.append(nn.Linear(in_features, len(np.unique(y))))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 6) Define Optuna objective\n",
        "# ----------------------------------------------------------------\n",
        "def objective(trial):\n",
        "    # Initialize history tracking\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    # Hyperparameter suggestions\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256)\n",
        "    n_layers    = trial.suggest_int(\"n_layers\", 1, 3)\n",
        "    dropout     = trial.suggest_float(\"dropout\", 0.2, 0.5) # Use suggest_float\n",
        "    lr          = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True) # Use suggest_float with log=True\n",
        "    wd          = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True) # Use suggest_float with log=True\n",
        "    batch_size  = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    n_epochs    = 80 # Increase max epochs since we have early stopping\n",
        "    patience    = 10 # Early stopping patience\n",
        "\n",
        "    # Split train → sub-train + validation for this trial\n",
        "    X_sub, X_val, y_sub, y_val = train_test_split(\n",
        "        X_train_bal, y_train_bal,\n",
        "        test_size=0.2, stratify=y_train_bal, random_state=SEED\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_ds = TensorDataset(\n",
        "        torch.from_numpy(X_sub).float(),\n",
        "        torch.from_numpy(y_sub.values).long()\n",
        "    )\n",
        "    val_ds = TensorDataset(\n",
        "        torch.from_numpy(X_val).float(),\n",
        "        torch.from_numpy(y_val.values).long()\n",
        "    )\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Instantiate model, loss, optimizer\n",
        "    model = Net(input_dim=X_train.shape[1],\n",
        "                hidden_size=hidden_size,\n",
        "                n_layers=n_layers,\n",
        "                dropout=dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        for i, (xb, yb) in enumerate(train_loader):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "        history['train_loss'].append(epoch_train_loss / len(train_loader))\n",
        "\n",
        "        # Validation loss and accuracy\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                outputs = model(xb)\n",
        "                loss = criterion(outputs, yb)\n",
        "                val_loss += loss.item()\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                correct += (preds == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc = correct / total\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_model_state = model.state_dict() # Save best model state\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                # Restore best model weights before returning\n",
        "                model.load_state_dict(best_model_state)\n",
        "                break # Stop training\n",
        "\n",
        "\n",
        "    # Store history in trial user_attrs\n",
        "    trial.user_attrs['history'] = history\n",
        "\n",
        "    # Return the validation accuracy of the *best* model state encountered\n",
        "    model.load_state_dict(best_model_state) # Ensure model is at best state\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            outputs = model(xb)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    final_val_acc = correct / total\n",
        "\n",
        "    print(f\"[Trial {trial.number}] Epochs: {len(history['val_acc'])}\")\n",
        "    trial.set_user_attr(\"history\", history)\n",
        "\n",
        "    return final_val_acc # Return accuracy of the best model state\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 7) Run Optuna study\n",
        "# ----------------------------------------------------------------\n",
        "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(seed=SEED))\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "print(\"Best hyperparameters:\", study.best_params)\n",
        "print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
        "\n",
        "# ----------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li6lzdD9K0yV"
      },
      "outputs": [],
      "source": [
        "# 8) Train final model on full balanced train split using best params\n",
        "# ----------------------------------------------------------------\n",
        "best = study.best_params\n",
        "final_model = Net(\n",
        "    input_dim   = X_train.shape[1],\n",
        "    hidden_size = best[\"hidden_size\"],\n",
        "    n_layers    = best[\"n_layers\"],\n",
        "    dropout     = best[\"dropout\"]\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(final_model.parameters(),\n",
        "                       lr=best[\"lr\"],\n",
        "                       weight_decay=best[\"weight_decay\"])\n",
        "batch_size = best[\"batch_size\"]\n",
        "epochs = 30 # You might want to increase this and use early stopping here too\n",
        "\n",
        "full_ds = TensorDataset(\n",
        "    torch.from_numpy(X_train_bal).float(),\n",
        "    torch.from_numpy(y_train_bal.values).long()\n",
        ")\n",
        "full_loader = DataLoader(full_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Optional: Add early stopping for final model training as well\n",
        "# best_full_val_loss = float('inf')\n",
        "# epochs_no_improve_full = 0\n",
        "# patience_full = 10 # Same patience or different\n",
        "# best_full_model_state = None\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    final_model.train()\n",
        "    for xb, yb in full_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        loss = criterion(final_model(xb), yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # If you added early stopping for final model:\n",
        "    # Evaluate on X_val, y_val here (need to split X_train_bal, y_train_bal for this)\n",
        "    # val_loss_full = ...\n",
        "    # if val_loss_full < best_full_val_loss:\n",
        "    #     best_full_val_loss = val_loss_full\n",
        "    #     epochs_no_improve_full = 0\n",
        "    #     best_full_model_state = final_model.state_dict()\n",
        "    # else:\n",
        "    #     epochs_no_improve_full += 1\n",
        "    #     if epochs_no_improve_full >= patience_full:\n",
        "    #         final_model.load_state_dict(best_full_model_state)\n",
        "    #         print(f\"Early stopping at epoch {epoch}\")\n",
        "    #         break\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# 9) Evaluate on held-out X_test\n",
        "# ----------------------------------------------------------------\n",
        "# If early stopping was used for final model, ensure it's at best state\n",
        "# if best_full_model_state:\n",
        "#    final_model.load_state_dict(best_full_model_state)\n",
        "\n",
        "final_model.eval()\n",
        "X_test_tensor = torch.from_numpy(X_test).float().to(device)\n",
        "with torch.no_grad():\n",
        "    logits = final_model(X_test_tensor)\n",
        "    probs  = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    preds  = probs.argmax(axis=1)\n",
        "\n",
        "# Accuracy\n",
        "acc = accuracy_score(y_test, preds)\n",
        "print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Confusion matrix & classification report\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, preds))\n",
        "\n",
        "# ROC–AUC\n",
        "auc = roc_auc_score(y_test, probs[:,1])\n",
        "fpr, tpr, _ = roc_curve(y_test, probs[:,1])\n",
        "print(f\"\\nTest ROC–AUC: {auc:.4f}\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\")\n",
        "plt.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
        "plt.title(\"ROC Curve - ANN\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-MA8XQiIMaF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_top_trials_history(study, top_n=3):\n",
        "    valid_trials = [t for t in study.trials if 'history' in t.user_attrs]\n",
        "    if len(valid_trials) == 0:\n",
        "        print(\"No trials have training history.\")\n",
        "        return\n",
        "\n",
        "    top_trials = sorted(valid_trials, key=lambda t: t.value, reverse=True)[:top_n]\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    metrics = ['train_loss', 'val_loss', 'val_acc']\n",
        "    titles = ['Training Loss', 'Validation Loss', 'Validation Accuracy']\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        ax = axs[i]\n",
        "        for trial in top_trials:\n",
        "            history = trial.user_attrs['history']\n",
        "            values = history[metric]\n",
        "            ax.plot(range(1, len(values) + 1), values, label=f\"Trial {trial.number} (val_acc={trial.value:.4f})\")\n",
        "        ax.set_title(titles[i])\n",
        "        ax.set_xlabel(\"Epoch\")\n",
        "        ax.set_ylabel(metric.replace(\"_\", \" \").title())\n",
        "        ax.legend()\n",
        "        ax.grid(True)\n",
        "\n",
        "    fig.suptitle(f\"Top {top_n} Trials - Training History\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the plot function\n",
        "plot_top_trials_history(study, top_n=3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugzoLDLc6X5I"
      },
      "outputs": [],
      "source": [
        "from fairlearn.metrics import MetricFrame, demographic_parity_ratio, equalized_odds_ratio, selection_rate\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Suppress specific warnings from fairlearn\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"fairlearn\")\n",
        "\n",
        "# ---- Hàm tính False Positive Rate ----\n",
        "def false_positive_rate(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "# ---- Các metrics muốn đánh giá ----\n",
        "metrics = {\n",
        "    'accuracy': accuracy_score,\n",
        "    'selection_rate': selection_rate,\n",
        "    'recall': recall_score,\n",
        "    'precision': precision_score,\n",
        "    'false_positive_rate': false_positive_rate\n",
        "}\n",
        "\n",
        "# ---- Tạo MetricFrame cho ANN ----\n",
        "metric_frame_ann = MetricFrame(\n",
        "    metrics=metrics,\n",
        "    y_true=y_test_int,         # nhãn thật\n",
        "    y_pred=preds,              # dự đoán từ ANN\n",
        "    sensitive_features=sensitive_features_test\n",
        ")\n",
        "\n",
        "# ---- In kết quả theo group ----\n",
        "print(\"\\nFairness evaluation for ANN Model (Metrics by Group):\")\n",
        "display(metric_frame_ann.by_group)\n",
        "\n",
        "# ---- Tính Disparity ----\n",
        "print(\"\\nDisparities for ANN Model:\")\n",
        "\n",
        "# Demographic Parity Ratio\n",
        "dp_ratio_ann = demographic_parity_ratio(\n",
        "    y_true=y_test_int,\n",
        "    y_pred=preds,\n",
        "    sensitive_features=sensitive_features_test\n",
        ")\n",
        "print(\"Demographic parity ratio:\", dp_ratio_ann)\n",
        "\n",
        "# Equalized Odds Ratio\n",
        "eo_ratio_ann = equalized_odds_ratio(\n",
        "    y_true=y_test_int,\n",
        "    y_pred=preds,\n",
        "    sensitive_features=sensitive_features_test\n",
        ")\n",
        "print(\"Equalized odds ratio:\", eo_ratio_ann)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11cd089f"
      },
      "source": [
        "## Finish task\n",
        "The model training and evaluation is complete."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONKI8R+FCwIM+BsMWR6ZWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "caa542e0a3344ac6a20e47b29876bb1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5b65f879aae419481e4eca9710fbc89",
              "IPY_MODEL_67e3c50ef02d408385322d624145935a",
              "IPY_MODEL_1640e4c8723d4f399079967173ab632b"
            ],
            "layout": "IPY_MODEL_0e172a0b57524e079b1fa15b24cefb5d"
          }
        },
        "d5b65f879aae419481e4eca9710fbc89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8352a822f5c24067b92221ae95907ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_61daec16a49b4a009e99329031b9d73e",
            "value": "  0%"
          }
        },
        "67e3c50ef02d408385322d624145935a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6996fd562c834bd289d1ae8c434e09db",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5d4fadee42b4c4db3eab5d5f9b94918",
            "value": 0
          }
        },
        "1640e4c8723d4f399079967173ab632b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eee1882483224fccb7451509c6a35a71",
            "placeholder": "​",
            "style": "IPY_MODEL_5a5ec3d0330547138a41796b2d4bb229",
            "value": " 0/30 [00:00&lt;?, ?it/s]"
          }
        },
        "0e172a0b57524e079b1fa15b24cefb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8352a822f5c24067b92221ae95907ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61daec16a49b4a009e99329031b9d73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6996fd562c834bd289d1ae8c434e09db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5d4fadee42b4c4db3eab5d5f9b94918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eee1882483224fccb7451509c6a35a71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a5ec3d0330547138a41796b2d4bb229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}